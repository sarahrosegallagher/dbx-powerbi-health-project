{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0056d51-0b45-4f67-9037-7996a36f7e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Bronze Layer Ingestion Notebook (Workflow Ready)\n",
    "This notebook simulates production ingestion of raw Synthea data from cloud storage into Delta Lake Bronze tables. \n",
    "\n",
    "In a real production system, this notebook would be triggered automatically when a new file arrives in an AWS S3 bucket.\n",
    "\n",
    "---\n",
    "\n",
    "##  Conceptual Workflow (S3 â†’ Databricks Job Trigger)\n",
    "\n",
    "1. **Raw data file uploaded to S3**\n",
    "   - Location: `s3://my-bucket/synthea/raw/{table_name}.csv`\n",
    "\n",
    "2. **S3 Event Notification** triggers on `PUT` event\n",
    "\n",
    "3. **AWS Lambda** extracts the table name from file path and triggers this notebook via **Databricks Jobs API**\n",
    "   - Payload:\n",
    "     ```json\n",
    "     {\n",
    "       \"notebook_params\": {\n",
    "         \"table_name\": \"patients\"\n",
    "       }\n",
    "     }\n",
    "     ```\n",
    "\n",
    "4. **Databricks Job runs this notebook**, ingesting and cleaning the specific file, then saving as a Delta table in the Bronze layer.\n",
    "\n",
    "---\n",
    "\n",
    "##  Simulated Job Input\n",
    "Below, we use `dbutils.widgets` to simulate workflow input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921f86cb-50db-40cf-af19-f2c410f1ad09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 1: Set Workflow Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80378555-ab45-4410-94ec-7b5fecbacfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For workflow use: receive param from a job\n",
    "dbutils.widgets.text(\"table_name\", \"\")  # Empty by default\n",
    "param_table = dbutils.widgets.get(\"table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dacb3b3-ac6f-4a6c-97d0-6b717f6e2c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 2: Definte Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135a467a-2201-4f0f-a56e-b9d4666e8ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "import re\n",
    "\n",
    "# Clean column names using a consistent pattern\n",
    "def clean_column_name(col_name):\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '_', col_name).lower().strip('_')\n",
    "\n",
    "# Process a single CSV into a Delta-formatted Bronze table\n",
    "def process_bronze_table(table_name: str, source_dir=\"/FileStore/tables\", bronze_dir=\"/FileStore/bronze\"):\n",
    "    source_path = f\"{source_dir}/{table_name}.csv\"\n",
    "    bronze_path = f\"{bronze_dir}/{table_name}\"\n",
    "    bronze_table = f\"bronze_{table_name}\"\n",
    "\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "\n",
    "    # Load CSV\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .load(source_path)\n",
    "    )\n",
    "\n",
    "    # Clean column names\n",
    "    df = df.toDF(*[clean_column_name(c) for c in df.columns])\n",
    "\n",
    "    # Add metadata columns\n",
    "    df = df.withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "           .withColumn(\"source_file\", lit(source_path))\n",
    "\n",
    "    # Write Delta file to Bronze path\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "    # Register the Delta file as a Hive table\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {bronze_table}\")\n",
    "    spark.sql(f\"CREATE TABLE {bronze_table} USING DELTA LOCATION '{bronze_path}'\")\n",
    "\n",
    "    print(f\"Bronze table created: {bronze_table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c9ca94-3c56-482e-91dc-1196ef2a325d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Step 3: Process Table(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f1c6ca-7dd3-4fb5-a877-ee6d5cc00232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: patients\nBronze table created: bronze_patients\nProcessing table: encounters\nBronze table created: bronze_encounters\nProcessing table: providers\nBronze table created: bronze_providers\nProcessing table: conditions\nBronze table created: bronze_conditions\nProcessing table: organizations\nBronze table created: bronze_organizations\n"
     ]
    }
   ],
   "source": [
    "bronze_tables = [\"patients\", \"encounters\", \"providers\", \"conditions\", \"organizations\"]\n",
    "\n",
    "# Decide: run one table (workflow) or all (manual)\n",
    "if param_table:\n",
    "    process_bronze_table(param_table)\n",
    "else:\n",
    "    for table in bronze_tables:\n",
    "        process_bronze_table(table)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Synthea_Bronze_Load",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}